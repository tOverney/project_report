\documentclass{report}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\def \scalingfactor{.8}

\begin{document}

\title{Insert Appropriate Title Here}
\author{Cl\'{e}ment Humbert, Tristan Overney\and Paolo Ienne, Andrea Miele, Ewaida Moshen (Supervisors)}
\date{\today}
\maketitle

\nocite{*}

\setcounter{tocdepth}{1}
\tableofcontents
\listoffigures

\chapter{Introduction}

    Due to their SIMD nature, GPUs have been used in different scientific fields to perform
    non-graphical computations for a while. They're used for particle and molecular dynamic 
    modeling, astrophysic and fluid mechanic modeling.
    More recently they've been used for cryptographic applications and in particular
    to factorize large numbers in an attempt to compromise RSA encryption \cite{cofact14}.
    Cryptography-related applications make heavy use of very large integer operations.
    Here's an algorithm as example from \cite{cofact14}:
    \begin{algorithm}
    \caption{Large operands multiplication}\label{mulalgo}
    \begin{algorithmic}[1]
        \Require{Integers x and $Y=\sum_{i=0}^{n-1}Y_ir^i$ such that $0\leq x$, $Y_i<r$ for $0\leq i<n$.}
        \Ensure{$Z=x\cdot Y = \sum_{i=0}^nZ_ir^i$}
        \State mul.lo($Z_0$, x, $Y_0$)
        \State mul.hi($Z_1$, x, $Y_0$)
        \State mad.lo.cc($Z_1$, x, $Y_1$, $Z_1$)
        \State mul.hi($Z_2$, x, $Y_1$)
        \For{$i=2$ to $n-2$}
            \State madc.lo.cc($Z_i$, x, $Y_i$, $Z_i$)
            \State mul.hi($Z_{i+1}$, x, $Y_i$)
        \EndFor
        \State madc.lo.cc($Z_{n-1}$, x, $Y_{n-1}$, $Z_{n-1}$)
        \State madc.hi($Z_n$, x, $Y_{n-1}$, 0)
        \State \textbf{return} $Z (=\sum_{i=0}^nZ_ir^i)$
    \end{algorithmic}
    \end{algorithm}

    Due to the nature of asymetric cryptography, these applications are bound by
    integer computing capacity which is not the focus of GPU. GPUs are designed
    around floating-point operations.

\section{Goals}
    The aim of this project is to study and identify the limitations of GPUs
    when used for cryptography and large integers computation in order to suggest
    hardware changes with integer capabilities appropriate for integer-heavy applications.

\chapter{First approach}
    The first idea to enhance the cryptographic capabilities of the GPU is to
    build crypto specific hardware into the original hardware to expose new 
    operations (in the form of assembly opcodes) to the developers with the 
    shortest latency possible. 
    
    To do so different ways of implementing Montgomery multipliers were
    explored:
    %TODO INSERT MONTGOMERY SHIT HERE

    The impact of the new opcodes were to be measured with the assistance of
    GPGPU-SIM\cite{gpgpusim09} a Fermi architecture simulator. It was determined
    that new opcodes can be added to gpgpu-sim and included in an instruction group
    to modify its latency (an unused operation could have been chosen to set the new
    instruction's latency).

    To determine the speed/size of the new components the original cores have to
    be studied as not much is known from nVidia sources. In order to understand 
    the cores and be able to suggest meaningful changes, a serie of microbenchmarking 
    experiments had to be done. 


\chapter{Fermi's architecture}
    Before diving into the experiments and their results here's a quick-start
    guide to Nvidia's Fermi architecture and its vocabulary:
    %TODO add a picture of the full chip here? Or in the part where the cost increase is computed?
    \section{Streaming Multiprocessors}
    The largest building block inside the Fermi architecture is the 
    \emph{Streaming multiprocessor} (Figure ~\ref{fig:SM} on page 
    ~\pageref{fig:SM}) abbreviated SM in this report. The GTX580 card is equipped
    with 16 of these SMs.
    \begin{figure}[H]
    \centering
        \includegraphics[width=0.75\linewidth]{pictures/Fermi}
        \captionsetup{justification=centering}
        \caption{Fermi's streaming multiprocessor schematic representation.}
        \label{fig:SM}
    \end{figure}

    Each SM is composed of the following computation blocks:
    \begin{itemize}
        \item 32 CUDA cores in two groups of 16,
        \item 16 load/store units (LD/ST on the figure)
        \item 4 Special Functions Units (SFUs) dedicated to more complex
              arithmetic functions such as sines and logarithms.
    \end{itemize}
    
    As the SFU is dedicated to floating-point operations it's already an area
    that can be reused for integer computation components, thus it's not part
    of the following research. The focus of the experiments are the CUDA cores.

    \section{CUDA Cores}
    Figure~\ref{fig:CUDACore} is the representation of what a CUDA core is,
    according to Nvidia.
    
    \begin{figure}[H]
    \centering
        \includegraphics[width=0.5\linewidth]{pictures/CUDACore}
        \captionsetup{justification=centering}
        \caption{Schematic representation of Nvidia's CUDA Core.}
        \label{fig:CUDACore}
    \end{figure}
    
    From figure~\ref{fig:CUDACore} the notion of twice 16 cores can be
    adjusted to 2x16 integers ALUs and 2x16 simple-precision floating-point
    ALUs.

    \section{Basics of Fermi's scheduling}
        \subsection{Grids and blocks} 
        A cuda kernel (that is: a section of code that will run on the GPU) is
        launched with two parameters: a grid size and a block size. The grid 
        size is used to determine the number of SM on which the code will run.
        The block size determines how many threads will run the code inside each
        SM. As we're interested in the properties of single SMs or even, single
        cores, every benchmarking experiment was done using a grid size of one.

        \subsection{Warps}
        The second important scheduling unit is the warp. A warp consists
        of 32 threads. Each scheduling cycle, the scheduler selects two
        warps, for each of these it schedules 16 threads (making it 32
        threads scheduled, coming from two warps).

\chapter{Benchmarking}

\section{Methods}
	To get informations about the microarchitecture of the CUDA cores, a serie 
    of specially crafted CUDA kernels were used. These usually contain large 
    batches of instructions that were timed with the assistance of the \texttt{clock64()} 
    function offered by the CUDA API.

	The benchmark programs have been ran on a machine equipped with an Nvidia
    GeForce GTX 580 GPU.

\section{Pipeline properties}
	This section contains the results obtained through the previously described
	methods using large batches of integer multiplications.

	\subsection{Benchmark running times against number of threads}
	\label{par:pipeline_exp}
	\subsubsection{Description of the experiment}
	The first experiment aims at outlining the relation between the running
    times of the benchmark program and the number of threads running parallely in
    a single block (the threads reside on one SM).
	\subsubsection{Expectations}
    The running times are expected to be slightly higher for the integer
    multiplications, as GPUs are optimized for floting point computation
    but to deteriorate in a similar fashion (begin to deteriorate
    at the same point, at the same rate) due to each core being equipped with
    integer and single-precision floating-point ALUs.
    
	\subsubsection{Results and analysis}
    \begin{figure}[H]
    	\centering
		\vspace{-20pt}
	    \includegraphics[width=\scalingfactor\linewidth]{"graphics/float_vs_int_running_times"}
		\vspace{-15pt}
		\captionsetup{justification=centering}
        \caption{Running times of benchmark (in cycles) against number of threads.}
    \end{figure}
	
    The results are against the expectations as it's easy to see the integer
    multiplication times starting to grow at 289 threads in the block against
    577 for the simple-precision floating-point multiplications.
    
    %not at the right place, there should be a section about FP benchmark.
    As the running time is constant up to 576 threads running in parallel and that we have 32 cores
    available, the hypothesis has been formulated that the pipeline depth for the single-precision 
    floating-point multiplication is 18 ($576/32 = 18$). Which seems to correlate with the running
    times of the integers multiplication. 
    
    Figure~\ref{fig:fp_prediction_512}
    represents the hypothesis made about the pipeline depth when 576 threads are running, $C_1$ to
    $C_{32}$ are the core of the single SM on which the experiment has been run. (1 to 16 is the
    first core group and 17 to 32 is the second one.) $t_{j,k}$ is the $k$-eme instruction of the thread number $j$. 
        \begin{figure}[H]
      \centering
       \begin{tabular}{ | r || c | c | c | c || c | c | c | }
    	    \hline
    	    time since beginning & $C_1$ & $C_2$ & ... & $C_{16}$ & $C_{17}$ & ... & $C_{32}$ \\ \hline  \hline
    	   1 & $t_{0,0}$ & $t_{1,0}$ & ... & $t_{15,0}$ & $t_{32, 0}$ & ... & $t_{47, 0}$ \\ \hline 
    	   2 & $t_{16,0}$ & $t_{17,0}$ & ... & $t_{31,0}$ & $t_{48, 0}$ & ... & $t_{63, 0}$ \\ \hline
    	   3 & $t_{64,0}$ & $t_{65,0}$ & ... & $t_{79,0}$ & $t_{96, 0}$ & ... & $t_{111, 0}$ \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
    	   16 & $t_{528,0}$ & $t_{529,0}$ & ... & $t_{543,0}$ & $t_{560,0}$ & ... & $t_{575, 0}$ \\ \hline
    	   17 & $t_{0,1}$ & $t_{1,1}$ & ... & $t_{15,1}$ & $t_{32, 1}$ & ... & $t_{47, 1}$ \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Planned running scheme for 576 threads with floating point number multiplication.}
  	\label{fig:fp_prediction_512}
   \end{figure}
   
    Figure~\ref{fig:int_prediction_256} represents the hypothesis made about the pipeline when 288
    threads are running, $C_1$ to $C_{32}$ are the core of the single SM on which the experiment
    has been run. (1 to 16 is the first core group and 17 to 32 is the second one.). The first
    group core is provided in integer multiplication but the second. $t_{j,k}$ is the $k$-eme instruction of the thread number
    $j$.
   \begin{figure}[H]
      \centering
       \begin{tabular}{ | r || c | c | c | c || c | c | c | }
    	    \hline
    	    time since beginning & $C_1$ & $C_2$ & ... & $C_{16}$ & $C_{17}$ & ... & $C_{32}$ \\ \hline  \hline
    	   1 & $t_{0,0}$ & $t_{1,0}$ & ... & $t_{15,0}$ & - & ... & - \\ \hline 
    	   2 & $t_{16,0}$ & $t_{17,0}$ & ... & $t_{31,0}$ & - & ... & - \\ \hline
    	   3 & $t_{32,0}$ & $t_{33,0}$ & ... & $t_{47,0}$ & - & ... & - \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
    	   18 & $t_{272,0}$ & $t_{273,0}$ & ... & $t_{287,0}$ & - & ... & - \\ \hline
    	   19 & $t_{0,1}$ & $t_{1,1}$ & ... & $t_{15,1}$ & - & ... & - \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Planned running scheme for 288 threads with integer number multiplication.}
  	\label{fig:int_prediction_256}
   \end{figure}

    To understand these deterioration points the next experiment has been designed.

\section{Understanding pipeline length and performance deterioration}
    \subsection{The experiment}
    With the results of \ref{par:pipeline_exp} in mind, it's clear that the pipelines depth 
    must be 18. The objective of the following experiment is to determine the cost of
    the loop used in the benchmarks to adjust the running times found and see if they match with
    the hypothesis about pipeline length and scheduling. 
	\subsubsection{Expectations}
	As seen in \ref{par:pipeline_exp} the cost of simple-precision floating-point
    operations goes up at 576 threads. This would imply that every pipeline of
    the SM is perfectly filled with 576 threads. Dividing 576 threads by
    the 32 cores gives a pipeline length of 18. In addition, the cost increase
    is suspected to be 1/9 of the base cost as the scheduling is expected to be
    something like described in the following schematic:
    
    \begin{figure}[H]
      \centering
       \begin{tabular}{ | r || c | c | c | c || c | c | c | }
    	    \hline
    	    time since beginning & $C_1$ & $C_2$ & ... & $C_{16}$ & $C_{17}$ & ... & $C_{32}$ \\ \hline  \hline
    	   1 & $t_{0,0}$ & $t_{1,0}$ & ... & $t_{15,0}$ & $t_{32, 0}$ & ... & $t_{47, 0}$ \\ \hline 
    	   2 & $t_{16,0}$ & $t_{17,0}$ & ... & $t_{31,0}$ & $t_{48, 0}$ & ... & $t_{63, 0}$ \\ \hline
    	   3 & $t_{64,0}$ & $t_{65,0}$ & ... & $t_{79,0}$ & $t_{96, 0}$ & ... & $t_{111, 0}$ \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
    	   18 & $t_{528,0}$ & $t_{529,0}$ & ... & $t_{543,0}$ & $t_{560,0}$ & ... & $t_{575, 0}$ \\ \hline
    	   19 & $t_{576,0}$ & - & ... & - & - & ... & - \\ \hline
    	   20 & - & - & ... & - & - & ... & - \\ \hline
    	   21 & $t_{0,1}$ & $t_{1,1}$ & ... & $t_{15,1}$ & $t_{32,1}$ & ... & $t_{47,1}$ \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Planned running scheme for 577 threads with floating point number multiplication.}
  	\label{fig:fp_prediction_513}
   \end{figure}

    The 1/9 can be derived from the following formula
    \[ \cfrac{t_{\text{577threads}} - t_{\text{576threads}}}{t_{\text{576threads}}} \text{  .}\]
    The latency of an instruction is equal to the pipeline length, plus 1/18 of
    it for every additional warp after the $18^{\text{th}}$. So the latency l of an 
    operation is $L + \text{max}(0, \ceil{\cfrac{N}{32} -18})$ and to get the total
    running time of a benchmark, this latency must be added as a constant to
    represent the time the last instruction takes to get through the pipeline.

    The expansion of the previous formula gives
    \[ \cfrac{(18\cdot l_{576} + l_{577} + l_{577}) - (18\cdot l_{576} + l_{576})}{18\cdot l_{576} + l_{576}} \text{  .}\]
    Which, with numerical values and simplifications is
    \[ \cfrac{(18\cdot 19 + 2\cdot 19) - (18^2 + 18)}{18^2 + 18} = \cfrac{1}{9}\text{  .}\] 

    Thus, the outcome expected is that once the loop cost is removed from the 
    times obtained in \ref{par:pipeline_exp} the remaining time is 18 millions
    which is 18 cycles per operation, fitting the 18-stages pipeline hypothesis.
    The expected difference in running time after adjustments is 2 millions cycles,
    1/9 of 18 millions.

	\subsubsection{Results and analysis}
    \begin{figure}[H]
		\centering
		\vspace{-20pt}
    			\includegraphics[width=\scalingfactor\linewidth]{"graphics/for-sizes-superpositions"}
		\vspace{-15pt}
		\captionsetup{justification=centering}
		\caption{Running a million operations broken down in 10 and 1000 loop iterations.}
        \label{fig:for-cost}
	\end{figure}
    
    Figure ~\ref{fig:for-cost} suggests that only a insignificant amount of time is spent by the loop
    iterations' instructions. The next possible explanation would be data transfer overhead caused
    by compiler shenanigans.

\section{Mixing single-precision floating-points and integer multiplication}
	\subsection{The experiment}
	Information has been found in \ref{par:pipeline_exp} that was implying that the number of 
    integer multiplication able to run parallely on an SM was only half the number of 
    single-precision floating-point multiplication. Leading to the conclusion that only one of
    the two 16 cores group of an SM was equipped with integer. The following experiments issue 
    integer multiplication in parallel of floating-point operations to confirm this hypothesis.
	\subsection{Benchmark running times, 1 single-precision floating-points for 1 integer multiplication}
	If indeed only 1 out of 2 cores group can run integer multiplication then
    adding the same amount of floating-point multiplications as there were integer
    multiplications should not increase the total time spent executing the benchmark 
    program as the single-precision floating-point multiplication can
    be ran on the other core group (the one that does not possess integer multiplication).
	
	One million multiplication of each kind has been ran on 1 to 1024 threads to
    see if the results were comparable to the graph were there was only integer multiplication.
	%TODO Update that hideous graph!
	\begin{figure}[H]
		\centering
		\vspace{-20pt}
    			\includegraphics[width=\scalingfactor\linewidth]{"graphics/running_times_ratio11"}
		\vspace{-15pt}
		\captionsetup{justification=centering}
		\caption{Integer/Floating point multiplication ratio: 1.}
	\end{figure}
	\pagebreak

	\subsection{Benchmark running times with mixed single-precision floating-point and integer multiplications}
	\begin{figure}[H]
		\centering
		\vspace{-20pt}
        \includegraphics[width=\scalingfactor\linewidth]{"graphics/running_times_mixed"}
		\vspace{-15pt}
		\captionsetup{justification=centering}
        \caption{Running times of benchmarks with a mix of single-precision floating-points and integers multiplications.}
    \end{figure}

    \subsection{Results}
    The running times appear to be bound by those of the integer multiplication but is no higher than
    when only integer multiplications are ran, this confirms the hypothesis that only 16 of the 32
    CUDA core are equipped with integer ALUs. 

\section{Results}
    \subsection{Pipeline structure}
    As seen during the experiments, the CUDA core's pipeline appears to be an
    18 steps pipeline. The fact that integer multiplication and 
    simple-precision floating-point multiplication both take the same amount
    of time (on a machine that's supposed to be a floating-point calculation
    optimized device) until the pipelines are filled suggests a simple,
    no-dependency-check, scheduler that fires up new instructions every 18 cycles.

    It also appears rather clearly that, while 32 cores per SM are advertised by
    Nvidia, only 16 are equipped with integer ALUs; allowing only 18 integer
    operations to be scheduled every 18 cycles.
    
    \subsection{Prospects}
    From the previous constatations the following ideas are expected to drastically
    improve the integer computation performances while maintaining a stable (if not
    lower) cost in transistors:
    \begin{itemize}
        \item If any instruction is to be added (e.g.: Montgomery's multiplication, larger integer multiplication) these can take up to 18 cycles without having to modify any aspect of the scheduler.
        \item A large amount of integer computation power can be added at low-cost as a whole 16-cores group can be totally replaced by cores dedicated to integer arithmetic.
    \end{itemize}
\section{Additional graphics and tables}
	\subsection{Integer multiplication: 1024 threads starting times}
    \begin{figure}[H]
    		\centering
		\vspace{-20pt}
	    	\includegraphics[width=\scalingfactor\linewidth]{"graphics/starting_times_ratio31"}
	    	\vspace{-15pt}
	    	\captionsetup{justification=centering}    
	    	\caption{Order in which thread batches are started.}
    \end{figure}
   %TODO Fill the skeleton
\chapter{Beyond the current architecture}
    Given the knowledge acquired with all those experiments. The internal functioning of a Fermi SM was much clearer. And it was now possible to say what could possibly changed and whatnot.
   \section{Proposed enhancement}
   The most blatant upgrade that could be implemented would be to have the two core group to be identical in order to boost the issue rate of integer multiplication to a float multiplication-like performance.
   \begin{figure}[H]
      \centering
       \begin{tabular}{ | c | c | }
    	    \hline
    	    $1^{st}$ CUDA core group & $2^{nd}$ CUDA core group \\ \hline
    	   float unit and 32-bit multiplier & float unit \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Current CUDA core configuration.}
  	\label{fig:current_cores}
   \end{figure}
   There is two distinct approaches to this modification which are described as plan A and plan B in the following part of the report.
   \subsection{Plan A}
    In this first scenario, the change would be to add a 32-bit multiplier to every core that doesn't have one yet. So all 32 cores would have a full float unit and a 32-bit multiplier.
    \begin{figure}[H]
      \centering
       \begin{tabular}{ | c | c | }
    	    \hline
    	    $1^{st}$ CUDA core group & $2^{nd}$ CUDA core group \\ \hline
    	   float unit and 32-bit multiplier & float unit and 32-bit multiplier \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{First plan CUDA core new configuration.}
  	\label{fig:planA_cores}
   \end{figure}
    The expected outcome in term of issue rate would stay the same for float multiplication so 32 per clock cycle per SM. But the integer multiplication issue rate, which is assumed to have become the same as the float multiplication issue rate, is expected to also be 32 per clock cycle per SM.
    The advantage of this plan is that there is far less changes to the existing cores than plan B.
   \subsection{Plan B}
   In this second plan, instead of just adding 32-bit multiplier as in plan A; the change would be to have no more float support but instead have 32 integer only units with 32-bit multiplier.
   \begin{figure}[H]
      \centering
       \begin{tabular}{ | c | c | }
    	    \hline
    	    $1^{st}$ CUDA core group & $2^{nd}$ CUDA core group \\ \hline
    	   32-bit multiplier & 32-bit multiplier \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Second plan CUDA core new configuration.}
  	\label{fig:planB_cores}
   \end{figure}
   The expected outcome for that scenario would then be an issue rate of 0 per clock cycle per SM as the support for such operand type would be removed.
   And The integer multiplication issue rate is expected to be close to the current float multiplication issue rate which is 32 per clock cycle per SM.
   The advantage of this plan is that the new SM will be smaller than what it would be in plan B. 
   \section{Estimated performance increase}
    To estimate the increase in integers multiplications capabilities, gpgpusim
    is used. The base configuration is the one for GTX480 shipped with gpgpusim.
    With the default configuration, gpgpusim simulate integer multiplication as
    any other operation (with two pipelines) but with a slower initiation rate.
    The first step to measure the performance is to simulate a one-pipeline
    multiplication with the following configuration changes:
    \begin{figure}[H]
    \centering
       \begin{tabular}{ | l | l | }
            \hline
    	    Option & Value \\ \hline
    	    pipeline\_widths & 1,1,1,1,1,1,1 \\
            num\_sp\_units & 1 \\
            ptx\_opcode\_initiation\_int & 1,2,1,1,8 \\
            operand\_collector\_num\_in\_ports\_sp & 1 \\
            operand\_collector\_num\_out\_ports\_sp & 1 \\ \hline

  	    \end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Single multiplier simulation configuration.}
  	\label{tab:1stconfig}
    \end{figure}

    To simulate the changes, the following configuration is used (Table ~\ref{tab:2ndconfig}
    illustrates the changes from ~\ref{tab:1stconfig}):
    \begin{figure}[H]
    \centering
        \begin{tabular}{ | l | l | }
    	    \hline
    	    Option & Value \\ \hline
    	    pipeline\_widths & 2,1,1,2,1,1,2 \\
            num\_sp\_units & 2 \\
            ptx\_opcode\_initiation\_int & 1,2,2,1,8 \\
            operand\_collector\_num\_in\_ports\_sp & 2 \\
            operand\_collector\_num\_out\_ports\_sp & 2 \\ \hline
  	    \end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Two multipliers simulation configuration..}
  	\label{tab:2ndconfig}
    \end{figure}

    Launching an integer multiplication benchmark on both configurations yields the running
    times illustrated in ~\ref{graph_time_improvement}. 
    % TODO insert improvement graph

   \section{Estimated cost variance}
   In order to estimate the cost change a 24-bit multiplier and a 32-bit one have been synthesized with the 65nm library GPSVT.
   \[\text{24-bit multiplier} = mul_{24} = 10,354 \mu m^2 \text{ (in 65nm)}.\]
   \[\text{32-bit multiplier} = mul_{32} = 24,052 \mu m^2 \text{ (in 65nm)}.\]
   \subsection{Plan A}
   \subsection{Plan B}
   
\chapter{Conclusion} 
    This project was a mess, it's about everything there is to say!
    
    I'd like to thank Cl\'{e}ment Humbert for all the hard-work he put in this project and made it something else than just a huge "I don't care about that, leave me alone"  
   
   
  \appendix 
   
   %TODO put those stuff elsewhere!
\chapter{stuff that needs to be elsewhere}
    \section{Graphics intersteps data}
    The following tables describe the steps between running times in the graphics presented previously. Analyzing them may allow to deduce properties of: 
    \begin{itemize} 
        \item the cores' pipelines, if it represents the delay between dependencies checks;
        \item the scheduling mechanism, if it represents the delaying of threads operations in favor of the launch of other threads.
    \end{itemize}
    
    \centering
    \include{intersteps_times_1_SM_int}
    \include{intersteps_times_1_SM_float}
    \pagebreak
    

As there is no dependency between the even and odd instructions and there isn't any "new" thread to run, the scheduler looks for the next instruction of the threads belongings to the first two warps.
\begin{figure}[H]
      \centering
       \begin{tabular}{ | r || c | c | c | c || c | c | c | }
    	    \hline
    	    time since beginning & $C_1$ & $C_2$ & ... & $C_{16}$ & $C_{17}$ & ... & $C_{32}$ \\ \hline  \hline
    	   1 & $t_{0,0}$ & $t_{1,0}$ & ... & $t_{15,0}$ & $t_{32, 0}$ & ... & $t_{47, 0}$ \\ \hline 
    	   2 & $t_{16,0}$ & $t_{17,0}$ & ... & $t_{31,0}$ & $t_{48, 0}$ & ... & $t_{63, 0}$ \\ \hline
    	   3 & $t_{0,1}$ & $t_{1,1}$ & ... & $t_{15,1}$ & $t_{32, 1}$ & ... & $t_{47, 1}$ \\ \hline
           4 & $t_{16,1}$ & $t_{17,1}$ & ... & $t_{31,1}$ & $t_{48, 1}$ & ... & $t_{63, 1}$ \\ \hline
    	   5 & - & - & ... & - & - & ... & - \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
    	   18 & - & - & ... & - & - & ... & - \\ \hline
    	   19 & $t_{0,2}$ & $t_{1,2}$ & ... & $t_{15,2}$ & $t_{32,2}$ & ... & $t_{47,2}$ \\ \hline
    	   ... & ... & ... & ... & ... & ... & ... & ... \\ \hline
  	\end{tabular}
  	\captionsetup{justification=centering}
  	\caption{Planned running scheme for 64 threads with interleaved dependent floating point number multiplication.}
  	\label{fig:fp_prediction_64_halfdep}
   \end{figure}
   
   \bibliography{project_report}{}
   \bibliographystyle{plain}


\end{document}
